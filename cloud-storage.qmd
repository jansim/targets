---
execute:
  freeze: auto
---

# Cloud storage {#cloud-storage}

:::{.callout-warning}
## Cost

[Amazon S3](https://aws.amazon.com/s3/) or [Google Cloud Storage](https://cloud.google.com/storage/) are paid services. Amazon and Google not only charge for data, but also for operations that query or modify that data. Read <https://aws.amazon.com/s3/pricing/> and <https://cloud.google.com/storage/pricing> for details.
:::

:::{.callout-tip}
## Package version

This chapter requires `targets` version 1.3.0 or higher. Please visit the [installation instructions](https://docs.ropensci.org/targets/#installation).
:::

`targets` can store [data and metadata](#data) on the cloud, either with [Amazon Web Service (AWS) Simple Storage Service (S3)](https://aws.amazon.com/s3/) or [Google Cloud Platform (GCP) Google Cloud Storage (GCS)](https://cloud.google.com/storage/).

## Benefits

### Store less data locally

1. Use `tar_option_set()` and `tar_target()` to opt into cloud storage and configure options.
1. `tar_make()` uploads target data to a cloud bucket *instead of* the local [`_targets/objects/`](#data) folder. Likewise for [file targets](https://books.ropensci.org/targets/data.html#external-files).^[For cloud targets, `format = "file_fast"` has no purpose, and it automatically switches to `format = "file"`.]
1. Every `seconds_meta` seconds, `tar_make()` uploads metadata and still keeps local copies in [`_targets/meta/`](#data) folder.^[Metadata snapshots are synchronous, so a long target with `deployment = "main"` may block the main R process and delay uploads.]

### Inspect the results on a different computer

1. `tar_meta_download()` downloads the latest metadata from the bucket to the local [`_targets/meta/`](#data) folder.^[Functions `tar_meta_upload()`, `tar_meta_sync()`, and `tar_meta_delete()` also manage cloud metadata.] This allows you to interactively inspect the downloaded snapshot of the pipeline using `tar_visnetwork()`, `tar_progress()`, etc.^[Requires that the `repository_meta` option (in `tar_option_get()`) is not equal to `"local"` in `_targets.R`]
1. Helpers like `tar_read()` read local metadata and access target data in the bucket.
1. As of `targets` version `1.10.1.9002`, `tar_make()` uploads [debugging workspaces](https://books.ropensci.org/targets/debugging.html#workspaces) to the cloud. `tar_workspace_download()` can download a workspace file so you can load it locally with `tar_workspace()`.^[Requires that the `repository_meta` option (in `tar_option_get()`) is not equal to `"local"` in `_targets.R`]

### Track history

1. [Turn on versioning](https://docs.aws.amazon.com/AmazonS3/latest/userguide/manage-versioning-examples.html) in your [bucket](https://aws.amazon.com/s3/).
1. `tar_make()` records the versions of the target data in `_targets/meta/meta`.
1. Commit `_targets/meta/meta` to the same [version-controlled repository](https://github.com) as your R code.
1. Roll back to a [prior commit](https://git-scm.com/docs/git-commit) to roll back the [local metadata](#data) and give `targets` access to prior versions of the target data.

## Setup

### AWS setup

Skip these steps if you already have an [AWS](https://aws.amazon.com) account and [bucket](https://aws.amazon.com/s3/).

1. Sign up for a free tier account at <https://aws.amazon.com/free>. 
1. Read the [Simple Storage Service (S3) instructions](https://docs.aws.amazon.com/AmazonS3/latest/gsg/GetStartedWithS3.html) and practice in the [web console](https://console.aws.amazon.com/s3/). 
1. Install the `paws.storage` R package: `install.packages("paws.storage")`.
1. Follow the [`paws` documentation](https://www.paws-r-sdk.com/#credentials) to set your [AWS security credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/security-creds.html).
1. Create an [S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html), either in the [web console](https://console.aws.amazon.com/s3/) or with `paws.storage::s3()$create_bucket()`.

### GCP setup

Skip these steps if you already have an [GCP](https://cloud.google.com) account and [bucket](https://cloud.google.com/storage/).

1. Activate a Google Cloud Platform account at <https://cloud.google.com>.
1. Install the `googleCloudStorageR` R package: `install.packages("googleCloudStorageR")`.
1. Follow the [`googleCloudStorageR` setup instructions](https://code.markedmondson.me/googleCloudStorageR/articles/googleCloudStorageR.html) to authenticate into Google Cloud and enable required APIs.
1. Create a [Google Cloud Storage (GCS)](https://cloud.google.com/storage/) bucket either in the [web console](https://cloud.google.com/storage/) or `googleCloudStorageR::gcs_create_bucket()`. 

### Pipeline setup

Use `tar_option_set()` to opt into cloud storage and declare options. For AWS:^[`cue = tar_cue(file = FALSE)` is no longer recommended for [cloud storage](https://books.ropensci.org/targets/data.html#cloud-storage). This unwise shortcut is no longer necessary, as of <https://github.com/ropensci/targets/pull/1181> (`targets` version >= 1.3.2.9003).]

1. `repository = "aws"`
1. `resources = tar_resources(aws = tar_resources_aws(bucket = "YOUR_BUCKET", prefix = "YOUR/PREFIX"))`

Details:

* The process is analogous for GCP.
* The `prefix` is just like `tar_config_get("store")`, but for the cloud. It controls where the data objects live in the bucket, and it should not conflict with other projects. 
* Arguments `repository`, `resources`, and `cue` of `tar_target()` override their counterparts in `tar_option_set()`.
* In `tar_option_set()`, `repository` controls the target data, and `repository_meta` controls the metadata. However, `repository_meta` just defaults to `repository`. To continuously upload the metadata, it usually suffices to set e.g. `repository = "aws"` in `tar_option_set()`.

## Example

Consider a pipeline with two simple targets.

```{r, eval = FALSE, echo = TRUE}
# Example _targets.R file:
library(targets)
library(tarchetypes)

tar_option_set(
  repository = "aws",
  resources = tar_resources(
    aws = tar_resources_aws(
      bucket = "my-test-bucket-25edb4956460647d",
      prefix = "my_project_name"
    )
  )
)

write_file <- function(data) {
  saveRDS(data, "file.rds")
  "file.rds"
}

list(
  tar_target(data, rnorm(5), format = "qs"), 
  tar_target(file, write_file(data), format = "file")
)
```

As usual, `tar_make()` runs the correct targets in the correct order. Both data files now live in bucket `my-test-bucket-25edb4956460647d` at S3 key paths which begin with [prefix](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-prefixes.html) `my_project_name`. Neither `_targets/objects/data` nor `file.rds` exist locally because `repository` is `"aws"`.

```{r, eval = FALSE, echo = TRUE}
tar_make()
#> ▶ start target data
#> ● built target data [0 seconds]
#> ▶ start target file
#> ● built target file [0.002 seconds]
#> ▶ end pipeline [1.713 seconds]
```

At this point, if you switch to a different computer, download your metadata with `tar_meta_download()`. Then, your results will be up to date.

```{r, eval = FALSE, echo = TRUE}
tar_make()
#> ✔ skip target data
#> ✔ skip target file
#> ✔ skip pipeline [1.653 seconds]
```

`tar_read()` read local metadata and cloud target data.

```{r, eval = FALSE, echo = TRUE}
tar_read(data)
#> [1] -0.74654607 -0.59593497 -1.57229983  0.40915323  0.02579023
```

For a file target, `tar_read()` downloads the file to its original location and returns the path.

```{r, eval = FALSE, echo = TRUE}
path <- tar_read(file)
path
#> [1] "file.rds"
readRDS(path)
#> [1] -0.74654607 -0.59593497 -1.57229983  0.40915323  0.02579023
```
